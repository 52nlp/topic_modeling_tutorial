{
 "metadata": {
  "name": "",
  "signature": "sha256:9b5d9baf29f6b6cddbcebb4d1925389b225042d269b34116e7f80c388bc16f36"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Topic Modeling for Fun and Profit"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we'll\n",
      "\n",
      "* vectorize a streamed corpus\n",
      "* run topic modeling on streamed vectors, using gensim\n",
      "* explore how to choose, evaluate and tweak topic modeling parameters\n",
      "* persist trained models to disk, for later re-use"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous notebook `1 - Streamed Corpora` we used the 20newsgroups corpus to demonstrate data preprocessing and streaming.\n",
      "\n",
      "Now we'll switch to the English Wikipedia and do some topic modeling."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import and setup modules we'll be using in this notebook\n",
      "import logging\n",
      "import itertools\n",
      "\n",
      "import gensim\n",
      "\n",
      "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
      "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Wikipedia corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use the now-familiar pattern of streaming over an entire Wikipedia dump, without unzipping the raw file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.utils import smart_open, simple_preprocess\n",
      "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
      "from gensim.parsing.preprocessing import STOPWORDS\n",
      "\n",
      "def iter_wiki(dump_file):\n",
      "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
      "    for title, text, pageid in _extract_pages(smart_open(dump_file)):\n",
      "        text = filter_wiki(text)\n",
      "        tokens = [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
      "        if len(tokens) < 50 or any(title.startswith(ignore + ':') for ignore in ignore_namespaces):\n",
      "            continue\n",
      "        yield tokens\n",
      "\n",
      "# only use simplewiki in this tutorial (fewer documents)\n",
      "# the full wiki dump is exactly the same format, but larger\n",
      "stream = iter_wiki('./data/simplewiki-20140623-pages-articles.xml.bz2')\n",
      "print(list(itertools.islice(stream, 2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dictionaries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dictionaries are objects that map into raw text tokens (strings) from their numerical ids (integers). Example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "id2word = {0: u'word', 2: u'profit', 300: u'another_word'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This mapping step is technically (not conceptually) necessary because most algorithms rely on numerical libraries that work with vectors indexed by integers, rather than by strings.\n",
      "\n",
      "The mapping can be constructed automatically by giving `Dictionary` class a stream of tokenized documents:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time id2word_wiki = gensim.corpora.Dictionary(iter_wiki('./data/simplewiki-20140623-pages-articles.xml.bz2'))\n",
      "print(id2word_wiki)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dictionary contains all words that appeared in the corpus, along with how many times they appeared. Let's filter out both very infrequent words and very frequent words (stopwords), to clear up resources as well as remove noise:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ignore words that appear in less than 20 documents or more than 10% documents\n",
      "id2word_wiki.filter_extremes(no_below=20, no_above=0.1)\n",
      "print(id2word_wiki)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise**: FIXME"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note for advanced users**: In fully online scenarios, where the documents can only be streamed once (no repeating the stream), we can't exhaust the stream just to build a dictionary. In this case we can map strings directly into their integer hash, using a hashing function such as MurmurHash or MD5. Such mapping is more difficult to debug, because the mapping is only one-way -- while we can retrieve the id (hash) for a given word, we cannot map an id to its word(s). See the documentation of [HashDictionary](http://radimrehurek.com/gensim/corpora/hashdictionary.html) for more details.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Vectorization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A streamed corpus and a dictionary is all we need to create [bag-of-words](http://en.wikipedia.org/wiki/Bag-of-words_model) vectors:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class WikiCorpus(object):\n",
      "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
      "        \"\"\"\n",
      "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
      "        Yield each document in turn, as a list of tokens (unicode strings).\n",
      "        \n",
      "        \"\"\"\n",
      "        self.dump_file = dump_file\n",
      "        self.dictionary = dictionary\n",
      "        self.clip_docs = clip_docs\n",
      "    \n",
      "    def __iter__(self):\n",
      "        for tokens in itertools.islice(iter_wiki(self.dump_file), 0, self.clip_docs):\n",
      "            yield self.dictionary.doc2bow(tokens)\n",
      "    \n",
      "    def __len__(self):\n",
      "        return self.clip_docs\n",
      "\n",
      "# create a stream of bag-of-words vectors\n",
      "wiki_corpus = WikiCorpus('./data/simplewiki-20140623-pages-articles.xml.bz2', id2word_wiki, clip_docs=8000)\n",
      "vector = next(iter(wiki_corpus))\n",
      "print(vector)  # print the first vector in the stream"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's store those vectors into a file, so we don't have to parse the Wikipedia XML every time over and over:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gensim.corpora.MmCorpus.serialize('./data/wiki.mm', wiki_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wiki_corpus = gensim.corpora.MmCorpus('./data/wiki.mm')\n",
      "print(wiki_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`wiki_corpus` now contains exactly the same vectors as before, but they are backed by the `.mm` file, rather than extracted on the fly from the `xml.bz2` file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(next(iter(wiki_corpus)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Semantic transformations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Topic modeling in gensim is realized via transformations. Let's train a model, using our WikiCorpus as training data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time lda_model = gensim.models.LdaModel(wiki_corpus, num_topics=10, id2word=id2word_wiki, passes=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_ = lda_model.print_topics(-1)  # print all 10 LDA topics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Transformation can be stacked. For example, here we'll train a TFIDF model, and then train [Latent Semantic Analysis](http://en.wikipedia.org/wiki/Latent_semantic_analysis) on top of TFIDF:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time tfidf_model = gensim.models.TfidfModel(wiki_corpus, id2word=id2word_wiki)\n",
      "%time lsi_model = gensim.models.LsiModel(tfidf_model[wiki_corpus], id2word=id2word_wiki, num_topics=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `model[corpus]` syntax (e.g. `tfidf_model[wiki_corpus]` above) applies a transformation on a corpus, returning another corpus.\n",
      "\n",
      "As always, the transformations are applied \"lazily\", so the resulting corpus is streamed as well:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(next(iter(lsi_model[tfidf_model[wiki_corpus]])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can store this \"LSA via TFIDF via bag-of-words\" corpus the same way again:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gensim.corpora.MmCorpus.serialize('./data/wiki_lsa.mm', lsi_model[tfidf_model[wiki_corpus]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Persisting the corpus to disk makes sense if we want to iterate over the corpus multiple times. As before, the result is indistinguishable from when it's computed on the fly, so this is conceptually a form of corpus caching:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi_corpus = gensim.corpora.MmCorpus('./data/wiki_lsa.mm')\n",
      "# `lsi_corpus` is now exactly the same stream as `lsi_model[tfidf_model[wiki_corpus]]`, but precomputed\n",
      "print(lsi_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More info on model parameters in [gensim docs](http://radimrehurek.com/gensim/models/lsimodel.html)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model persistence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gensim objects have `save/load` methods for persisting a model to disk, so it can be re-used later (or sent over network to a different computer, or whatever):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# store the model to disk\n",
      "lda_model.save('./data/lda_wiki.model')\n",
      "\n",
      "# load the same model back\n",
      "same_lda_model = gensim.models.LdaModel.load('./data/lda_wiki.model')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These methods are optimized for storing large models; internal matrices that consume a lot of RAM are [mmap](http://en.wikipedia.org/wiki/Mmap)'ed in read-only mode. This allows \"sharing\" a single model between several processes, through the OS's virtual memory management, saving all physical memory except for a single copy."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Eyeballing; perplexity; doppleganger. Performance on superordinate task (similarity)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Summary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* create an id => word mapping, aka dictionary\n",
      "* transform a document stream into a vector stream\n",
      "* transform between vector streams, using topic models\n",
      "* store and save models, for persistency"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Next"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the next notebook, we'll see how to index (transformed) corpora and run semantic queries.\n",
      "\n",
      "Continue by opening the next ipython notebook, `3 - Indexing and Retrieval`."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}