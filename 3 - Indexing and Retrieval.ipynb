{
 "metadata": {
  "name": "",
  "signature": "sha256:67379144127c82e9e3ccc580f92304ade569236c19a93d4f72b3c627476d5aed"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Topic Modeling for Fun and Profit"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook:\n",
      "    \n",
      "* index the documents in their semantic representation (topics)\n",
      "* retrieve \"most similar documents\" efficiently\n",
      "* write a small server for serving similarities over HTTP"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import & logging preliminaries\n",
      "import logging\n",
      "import itertools\n",
      "\n",
      "import gensim\n",
      "from gensim.parsing.preprocessing import STOPWORDS\n",
      "\n",
      "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
      "logging.root.level = logging.INFO\n",
      "\n",
      "def tokenize(text):\n",
      "    return [token for token in gensim.utils.simple_preprocess(text) if token not in STOPWORDS]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load the corpora created in the previous notebook\n",
      "tfidf_corpus = gensim.corpora.MmCorpus('./data/wiki_tfidf.mm')\n",
      "lsi_corpus = gensim.corpora.MmCorpus('./data/wiki_lsa.mm')\n",
      "print(tfidf_corpus)\n",
      "print(lsi_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load the models too\n",
      "id2word_wiki = gensim.corpora.Dictionary.load('./data/wiki.dictionary')\n",
      "lda_model = gensim.models.LdaModel.load('./data/lda_wiki.model')\n",
      "tfidf_model = gensim.models.TfidfModel.load('./data/tfidf_wiki.model')\n",
      "lsi_model = gensim.models.LsiModel.load('./data/lsi_wiki.model')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Document indexing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gensim contains three classes for indexing:\n",
      "\n",
      "* `gensim.similarities.MatrixSimilarity`: for an efficient, memory-mapped index  -- dense NumPy implementation\n",
      "* `gensim.similarities.SparseMatrixSimilarity`: for an efficient, memory-mapped index -- sparse SciPy implementation\n",
      "* `gensim.similarities.Similarity`: for an efficient out-of-core sharded index (auto-selects `MatrixSimilarity` or `SparseMatrixSimilarity` for each shard internally, based on the shard density); **this is the most flexible class and should be your first choice**.\n",
      "\n",
      "Let's see each in action:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.similarities import MatrixSimilarity, SparseMatrixSimilarity, Similarity\n",
      "\n",
      "# initialize the index\n",
      "%time index_dense = MatrixSimilarity(lsi_corpus, num_features=lsi_corpus.num_terms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time index_sparse = SparseMatrixSimilarity(tfidf_corpus, num_features=tfidf_corpus.num_terms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time index = Similarity('./data/wiki_index', lsi_corpus, num_features=lsi_corpus.num_terms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Adding new documents to the index"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `Similarity` class supports adding new documents to the index dynamically:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(index)\n",
      "# add the same documents into the index again, effectively doubling its size\n",
      "index.add_documents(lsi_corpus)\n",
      "print(index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise (5min)**: convert the document `text = \"A blood cell, also called a hematocyte, is a cell produced by hematopoiesis and normally found in blood.\"` into LSI space and index it into `index`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The documents have to come from the same semantic space (=the same model), of course. You can't mix bag-of-words with LDA or LSI documents in the same index.\n",
      "\n",
      "The other two classes, `MatrixSimilarity` and `SparseMatrixSimilarity`, don't support dynamic additions. The documents you supply during their construction is all they'll ever contain.\n",
      "\n",
      "All indexes can be persisted to disk using the now-familiar `save`/`load` syntax:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# store to disk\n",
      "index.save('./data/wiki_index.index')\n",
      "\n",
      "# load back = same index\n",
      "index = Similarity.load('./data/wiki_index.index')\n",
      "print(index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Semantic queries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An index can be used to find \"most similar documents\":"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query = \"April is the fourth month of the year, and comes between March \\\n",
      "and May. It has 30 days. April begins on the same day of week as July in \\\n",
      "all years and also January in leap years.\"\n",
      "\n",
      "# vectorize the text into bag-of-words and tfidf\n",
      "query_bow = id2word_wiki.doc2bow(tokenize(query))\n",
      "query_tfidf = tfidf_model[query_bow]\n",
      "query_lsi = lsi_model[query_tfidf]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# query the TFIDF index\n",
      "index_sparse.num_best = None\n",
      "print(index_sparse[query_tfidf])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This output is an array with one float per one indexed doc. Each float tells how similar the query is to that document. The higher the similarity score, the more similar the query to the document at the given index. The particular similarity measure used is [cosine similarity](http://en.wikipedia.org/wiki/Cosine_similarity).\n",
      "\n",
      "We can also request only the top-N most similar documents:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index_sparse.num_best = 3\n",
      "print(index_sparse[query_tfidf])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's find the five most similar articles according to `MatrixSimilarity` LSI index:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# query the LSI index\n",
      "index_dense.num_best = 5\n",
      "print(index_dense[query_lsi])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`Similarity` index works exactly the same:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index.num_best = 10\n",
      "print(index[query_lsi])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(you should see an identical result, except each top-N result is duplicated here, because we indexed the same LSI corpus twice into `index` a few lines above)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The size of the index structures scales linearly with the number of non-zero features in the corpus. For example, a dense LSI corpus of 1 million documents and 200 topics will consume ~800MB. Querying is fairly fast if you have a fast BLAS library installed:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(index)\n",
      "%timeit index[query_lsi]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "LSI demo on the full Wikipedia"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I've recently benchmarked available Python libraries for similarity search in high-dimensional spaces. For the benchmark I used the full English Wikipedia (~3.5 million documents), using code very similar to what we've done in these tutorials.\n",
      "\n",
      "There's also a frontend web app for this index, which lets you find similar Wikipedia articles from your browser: http://radimrehurek.com/2014/01/performance-shootout-of-nearest-neighbours-querying/#wikisim\n",
      "\n",
      "You can see the exact pipeline, including the web server part, [at github](https://github.com/piskvorky/sim-shootout)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Summary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook, we learned how to:\n",
      "\n",
      "* index arbitrary corpora\n",
      "* ask for top-N most similar documents, using the index\n",
      "* add new documents to a `Similarity` index\n",
      "* load and save indexes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Next"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's the end of this tutorial! Congratulations :)\n",
      "\n",
      "We skipped some more advanced topics like parallelization / distributed computations, and some advanced models, but you should have a clear idea of how streamed input works in gensim.\n",
      "\n",
      "If you have questions, use the [gensim mailing list](https://groups.google.com/d/forum/gensim).\n",
      "\n",
      "Gensim resides on [github](https://github.com/piskvorky/gensim) and has a [homepage with API reference, tutorials, testimonials](http://radimrehurek.com/gensim/) etc. Happy coding!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}